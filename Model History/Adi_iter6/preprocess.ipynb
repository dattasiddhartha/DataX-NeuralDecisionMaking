{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and initialize directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "from scipy.io import loadmat\n",
    "import unittest\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib\n",
    "\n",
    "import mne\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "beh_dir = '../../data/decision-making/data/data_behav'\n",
    "neur_dir = '../../data/decision-making/data/data_ephys'\n",
    "preproc_dir = '../../data/decision-making/data/data_preproc'\n",
    "\n",
    "beh_files = [file for file in glob.glob(os.path.join(beh_dir,\"gamble.data*.csv\"))]\n",
    "neur_files = [file for file in glob.glob(os.path.join(neur_dir,\"*.mat\"))]\n",
    "\n",
    "sfreq = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off interactive mode to use plt.psd just to get data, not to plot PSD for each trial\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_first = 0\n",
    "sample_last = 950\n",
    "num_samples = sample_last-sample_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create df where each row corresponds to 1 subject-electrode-trial = 1 example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is meaningful variation in power up to 50 Hz, and after that, it's qualitatively different, probably noise. I will extract analytic amplitude in 2 Hz bins up to 50 Hz, and use as features. See 0-preprocess-explore notebook for the images that generated this conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read bad_trials, for exclusion\n",
    "bad_trials = sio.loadmat(os.path.join(beh_dir, 'bad_trials_OFC.mat'))['bad_trials_OFC']\n",
    "\n",
    "# read game_model which, we hope, is identical across subjects\n",
    "game_model = pd.read_csv(os.path.join(beh_dir,'gamble_choices.csv'))\n",
    "\n",
    "# make master df where you append each subject's df\n",
    "df_master = pd.DataFrame(columns = ['subject', \n",
    "                                    'include.trial', \n",
    "                                    'round', \n",
    "                                    'newround.time', \n",
    "                                    'choice.time',\n",
    "                                    'buttonpress.time', \n",
    "                                    'conf.time', \n",
    "                                    'reveal.time', \n",
    "                                    'choice.class',\n",
    "                                    'choice.location', \n",
    "                                    'outcome', \n",
    "                                    'Safe.Bet.Amount', \n",
    "                                    'Risky.Bet.Amount',\n",
    "                                    'Risky.bet.shown.number', \n",
    "                                    'Risky.bet.hidden.number', \n",
    "                                    'Risky.Side',\n",
    "                                    'data', \n",
    "                                    'channel'])\n",
    "\n",
    "# Extract filtered data in 2-Hz bins from 0 to 50 Hz\n",
    "center_frequencies = np.linspace(1,49,num=25,dtype='int')\n",
    "\n",
    "for sub_index, files in enumerate(zip(beh_files, neur_files)):\n",
    "    beh_file = files[0]\n",
    "    neur_file = files[1]\n",
    "    \n",
    "    print(sub_index)\n",
    "    print()\n",
    "    \n",
    "    ## Read data\n",
    "    # ------------------------------------------------------------------------------------------------------.\n",
    "    # behavior\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_csv(os.path.join(beh_file))\n",
    "    \n",
    "    # neural\n",
    "    neur = loadmat(neur_file)['buttonpress_events_hg']\n",
    "    \n",
    "    ## Number trials and number electrodes\n",
    "    # ------------------------------------------------------------------------------------------------------.\n",
    "    num_trials_beh = len(df)\n",
    "    \n",
    "    num_trials = neur.shape[0]\n",
    "    num_samples = neur.shape[1]\n",
    "    nchan = neur.shape[2]\n",
    "    \n",
    "    # add subject column on the left: make it be 1-indexed, corresponding to the subid's in the file\n",
    "    df.insert(0, 'subject', sub_index+1)\n",
    "    \n",
    "    ## Append game model data\n",
    "    # ------------------------------------------------------------------------------------------------------.\n",
    "    df = df.merge(game_model[:num_trials_beh], left_index=True, right_index=True)    \n",
    "\n",
    "    ## Exclude bad trials from entire df: Makes it easier to match with neural data\n",
    "    # ------------------------------------------------------------------------------------------------------.\n",
    "    df.insert(1, 'include.trial', (bad_trials[sub_index,:num_trials_beh]==0) & (df['choice.location']!='Timeout'))\n",
    "    # exclude trials (shorten df)\n",
    "    df = df[df['include.trial']]\n",
    "    \n",
    "    # create a new index that just counts up to the number of included trials, and corresponds to the neural data\n",
    "    df.insert(0, 'trial_index_subject', np.arange(num_trials))\n",
    "    df = df.set_index('trial_index_subject')\n",
    "    \n",
    "    ## Add neural data\n",
    "    # ------------------------------------------------------------------------------------------------------.\n",
    "    # initialize a data column, that will take a row of data subject-electrode-trial,\n",
    "    # so a 1d-array of the number of time points in the data\n",
    "    df = df.assign(data=None)\n",
    "    df = df.assign(channel=None)\n",
    "    df_subject = pd.DataFrame(columns = df.columns)\n",
    "    \n",
    "    # loop over electrodes\n",
    "    for this_chan in range(nchan):\n",
    "        # create a dataframe for this specific channel, containing the behavior data for this subject\n",
    "        df_chan = df.copy()\n",
    "        df_chan['channel'] = this_chan\n",
    "        # loop over trials\n",
    "        for this_trial in range(num_trials):\n",
    "            # insert data for each trial of df: the neural data for electrode 0, that trial\n",
    "            # FIRST 950 time points only, in format that can be tf-decomposed\n",
    "            data = neur[this_trial,:sample_last,this_chan].astype('float64').reshape(1,-1)\n",
    "            # Get TF-features, analytic amplitude at each time point, in 2 Hz-bins from 0 to 50 Hz\n",
    "            # initialize tf-array\n",
    "#             tf = np.zeros([len(center_frequencies), 950])\n",
    "#             for index, center_frequency in enumerate(center_frequencies):\n",
    "#                 # compute analytic amplitude using filter-hilbert method\n",
    "#                 # z-score within frequency bin\n",
    "#                 tf[index,:] = stats.zscore(np.abs(signal.hilbert(mne.filter.filter_data(data, sfreq, center_frequency-1, center_frequency+1)[0])))\n",
    "#             pass\n",
    "#             # include both the \"raw\" data and the flattened tf-representation\n",
    "#             df_chan.at[this_trial, 'data'] = list(np.hstack([data.flatten(),tf.flatten()]))\n",
    "            df_chan.at[this_trial, 'data'] = list(data.flatten())\n",
    "        df_subject = df_subject.append(df_chan)\n",
    "    df_master = df_master.append(df_subject)\n",
    "    \n",
    "    df_master.insert(0, 'index', np.arange(len(df_master)))\n",
    "    df_master = df_master.set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include only gambles where first number is 5 or 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use = df_master[(df_master['Risky.bet.shown.number']==5) | (df_master['Risky.bet.shown.number']==6)]\n",
    "df_use.insert(0, 'index_use', np.arange(len(df_use)))\n",
    "df_use = df_use.set_index('index_use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5698814409484724"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df_use['choice.class']=='Gamble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8772"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert frequency with maximal amplitude value as feature (takes a while to compute, so do it only for included gambles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n"
     ]
    }
   ],
   "source": [
    "for this_index in df_use.index:\n",
    "    if this_index%200==0:\n",
    "        print(this_index)\n",
    "    data = df_use.data[this_index]\n",
    "    this_psd = plt.psd(np.asarray(data), Fs=sfreq);\n",
    "    xfreqs = this_psd[1]\n",
    "    yvals = np.log(this_psd[0])\n",
    "    freq_with_max_amp = xfreqs[np.argmax(yvals)]\n",
    "    max_amp_in_freq = yvals.max()\n",
    "    df_use.at[this_index,'data'] = list(np.hstack([data,freq_with_max_amp,max_amp_in_freq,yvals[:50]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a *separate* X-matrix, and separate y-labels, for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure this is 951\n",
    "len(df_use.data[this_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(df_use)\n",
    "num_samples = len(df_use.data[this_index])\n",
    "# # extract all the listed data into an array\n",
    "X = np.empty([num_examples,num_samples])\n",
    "\n",
    "for this_example in range(num_examples):\n",
    "    X[this_example,:] = np.asarray(df_use['data'][this_example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check here that X has the right shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over subjects and store in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = set(df_use['subject'])\n",
    "\n",
    "Xdict = dict()\n",
    "ydict = dict()\n",
    "\n",
    "for this_subject in subjects:\n",
    "#     print(this_subject)\n",
    "    df_subject = df_use[df_use['subject']==this_subject]\n",
    "    num_examples = len(df_subject)\n",
    "    X = np.empty([num_examples,num_samples])\n",
    "\n",
    "    for this_example in range(num_examples):\n",
    "#         X[this_example,sample_first:sample_last] = np.asarray(df_use['data'][this_example])[sample_first:sample_last]\n",
    "        X[this_example,:] = np.asarray(df_use['data'][this_example])\n",
    "#     print(X.shape)\n",
    "    Xdict[this_subject] = X\n",
    "    ydict[this_subject] = df_subject['choice.class'].values\n",
    "#     print(len(ydict[this_subject]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check here that contents of Xdict and ydict have the right shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save X and y dictionaries (took a while to compute, so important to save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data','Xdict.pickle'), 'wb') as handle1:\n",
    "    pickle.dump(Xdict, handle1, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data','ydict.pickle'), 'wb') as handle2:\n",
    "    pickle.dump(ydict, handle2, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Xdict.pickle', 'ydict.pickle']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
